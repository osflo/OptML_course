{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running Python 3. Good job :)\n"
     ]
    }
   ],
   "source": [
    "# Check the Python version\n",
    "import sys\n",
    "if sys.version.startswith(\"3.\"):\n",
    "  print(\"You are running Python 3. Good job :)\")\n",
    "else:\n",
    "  print(\"This notebook requires Python 3.\\nIf you are using Google Colab, go to Runtime > Change runtime type and choose Python 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "  # Clone the entire repo to access the files.\n",
    "  !git clone -l -s https://github.com/epfml/OptML_course.git cloned-repo\n",
    "  %cd cloned-repo/labs/ex02/template/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "b, A = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples n =  10000\n",
      "Dimension of each sample d =  2\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples n = ', b.shape[0])\n",
    "print('Dimension of each sample d = ', A.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares Estimation\n",
    "Least squares estimation is one of the fundamental machine learning algorithms. Given an $ n \\times d $ matrix $A$ and a $ n \\times 1$ vector $b$, the goal is to find a vector $x \\in \\mathbb{R}^d$ which minimizes the objective function $$f(x) = \\frac{1}{2n} \\sum_{i=1}^{n} (a_i^\\top x - b_i)^2 = \\frac{1}{2n} \\|Ax - b\\|^2 $$\n",
    "\n",
    "In this exercise, we will try to fit $x$ using Least Squares Estimation. \n",
    "\n",
    "One can see the function is $L$ smooth with $L =\\frac1n\\|A^T A\\|  = \\frac1n\\|A\\|^2$ (Lemma 2.3 for the first equality, and a few manipulations for the second)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Objective Function\n",
    "Fill in the `calculate_objective` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_objective(Axmb):\n",
    "    \"\"\"Calculate the mean squared error for vector Axmb = Ax - b.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute mean squared error\n",
    "    return 1/2*np.mean(Axmb**2)\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute smoothness constant $L$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the spectral norm of A you can use np.linalg.norm(A, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_L(b, A):\n",
    "    \"\"\"Calculate the smoothness constant for f\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute ||A.T*A||\n",
    "    temp=np.linalg.norm(A.T@A,2)\n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute L = smoothness constant of f\n",
    "    L=1/A.shape[0]*temp\n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(b, A, x):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient and objective\n",
    "    Axmb=A@x-b\n",
    "    grad=A.T@Axmb/A.shape[0]\n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "    return grad, Axmb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(b, A, initial_x, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store x and objective func. values\n",
    "    xs = [initial_x]\n",
    "    objectives = []\n",
    "    x = initial_x\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and objective function\n",
    "        grad,Axmb=compute_gradient(b,A,x)\n",
    "        obj=calculate_objective(Axmb)\n",
    "        # ***************************************************\n",
    "        #raise NotImplementedError\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update x by a gradient descent step\n",
    "        # ***************************************************\n",
    "        x=x-gamma*grad\n",
    "        #raise NotImplementedError\n",
    "        # store x and objective function value\n",
    "        xs.append(x)\n",
    "        objectives.append(obj)\n",
    "        print(\"Gradient Descent({bi}/{ti}): objective={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=obj))\n",
    "\n",
    "    return objectives, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function with a naive step size through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): objective=2792.2367127591674\n",
      "Gradient Descent(1/49): objective=2264.6350560300057\n",
      "Gradient Descent(2/49): objective=1837.2777140793814\n",
      "Gradient Descent(3/49): objective=1491.1182670993776\n",
      "Gradient Descent(4/49): objective=1210.729115045574\n",
      "Gradient Descent(5/49): objective=983.6139018819921\n",
      "Gradient Descent(6/49): objective=799.6505792194914\n",
      "Gradient Descent(7/49): objective=650.6402878628656\n",
      "Gradient Descent(8/49): objective=529.9419518639996\n",
      "Gradient Descent(9/49): objective=432.1762997049176\n",
      "Gradient Descent(10/49): objective=352.98612145606046\n",
      "Gradient Descent(11/49): objective=288.8420770744868\n",
      "Gradient Descent(12/49): objective=236.8854011254118\n",
      "Gradient Descent(13/49): objective=194.80049360666106\n",
      "Gradient Descent(14/49): objective=160.711718516473\n",
      "Gradient Descent(15/49): objective=133.0998106934208\n",
      "Gradient Descent(16/49): objective=110.73416535674836\n",
      "Gradient Descent(17/49): objective=92.6179926340438\n",
      "Gradient Descent(18/49): objective=77.94389272865318\n",
      "Gradient Descent(19/49): objective=66.05787180528668\n",
      "Gradient Descent(20/49): objective=56.43019485735972\n",
      "Gradient Descent(21/49): objective=48.63177652953899\n",
      "Gradient Descent(22/49): objective=42.31505768400419\n",
      "Gradient Descent(23/49): objective=37.19851541912101\n",
      "Gradient Descent(24/49): objective=33.054116184565615\n",
      "Gradient Descent(25/49): objective=29.69715280457577\n",
      "Gradient Descent(26/49): objective=26.978012466783984\n",
      "Gradient Descent(27/49): objective=24.775508793172605\n",
      "Gradient Descent(28/49): objective=22.991480817547423\n",
      "Gradient Descent(29/49): objective=21.546418157290997\n",
      "Gradient Descent(30/49): objective=20.3759174024833\n",
      "Gradient Descent(31/49): objective=19.42781179108905\n",
      "Gradient Descent(32/49): objective=18.65984624585973\n",
      "Gradient Descent(33/49): objective=18.037794154223974\n",
      "Gradient Descent(34/49): objective=17.53393195999899\n",
      "Gradient Descent(35/49): objective=17.125803582676767\n",
      "Gradient Descent(36/49): objective=16.795219597045776\n",
      "Gradient Descent(37/49): objective=16.527446568684663\n",
      "Gradient Descent(38/49): objective=16.310550415712164\n",
      "Gradient Descent(39/49): objective=16.134864531804435\n",
      "Gradient Descent(40/49): objective=15.992558965839178\n",
      "Gradient Descent(41/49): objective=15.877291457407319\n",
      "Gradient Descent(42/49): objective=15.783924775577509\n",
      "Gradient Descent(43/49): objective=15.70829776329537\n",
      "Gradient Descent(44/49): objective=15.647039883346832\n",
      "Gradient Descent(45/49): objective=15.597421000588525\n",
      "Gradient Descent(46/49): objective=15.557229705554294\n",
      "Gradient Descent(47/49): objective=15.52467475657656\n",
      "Gradient Descent(48/49): objective=15.498305247904602\n",
      "Gradient Descent(49/49): objective=15.476945945880312\n",
      "Gradient Descent: execution time=0.012 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_objectives_naive, gradient_xs_naive = gradient_descent(b, A, x_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c809b9ef02bd4fd88beeabc7c2e07d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "from grid_search import *\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    # Generate grid data for visualization (parameters to be swept and best combination)\n",
    "    grid_x0, grid_x1 = generate_w(num_intervals=10)\n",
    "    grid_objectives = grid_search(b, A, grid_x0, grid_x1)\n",
    "    obj_star, x0_star, x1_star = get_best_parameters(grid_x0, grid_x1, grid_objectives)\n",
    "    \n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_objectives_naive, gradient_xs_naive, grid_objectives, grid_x0, grid_x1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_xs_naive)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try doing gradient descent with a better learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): objective=2792.2367127591674\n",
      "Gradient Descent(1/49): objective=15.3858878688294\n",
      "Gradient Descent(2/49): objective=15.3858878688294\n",
      "Gradient Descent(3/49): objective=15.3858878688294\n",
      "Gradient Descent(4/49): objective=15.3858878688294\n",
      "Gradient Descent(5/49): objective=15.3858878688294\n",
      "Gradient Descent(6/49): objective=15.3858878688294\n",
      "Gradient Descent(7/49): objective=15.3858878688294\n",
      "Gradient Descent(8/49): objective=15.3858878688294\n",
      "Gradient Descent(9/49): objective=15.3858878688294\n",
      "Gradient Descent(10/49): objective=15.3858878688294\n",
      "Gradient Descent(11/49): objective=15.3858878688294\n",
      "Gradient Descent(12/49): objective=15.3858878688294\n",
      "Gradient Descent(13/49): objective=15.3858878688294\n",
      "Gradient Descent(14/49): objective=15.3858878688294\n",
      "Gradient Descent(15/49): objective=15.3858878688294\n",
      "Gradient Descent(16/49): objective=15.3858878688294\n",
      "Gradient Descent(17/49): objective=15.3858878688294\n",
      "Gradient Descent(18/49): objective=15.3858878688294\n",
      "Gradient Descent(19/49): objective=15.3858878688294\n",
      "Gradient Descent(20/49): objective=15.3858878688294\n",
      "Gradient Descent(21/49): objective=15.3858878688294\n",
      "Gradient Descent(22/49): objective=15.3858878688294\n",
      "Gradient Descent(23/49): objective=15.3858878688294\n",
      "Gradient Descent(24/49): objective=15.3858878688294\n",
      "Gradient Descent(25/49): objective=15.3858878688294\n",
      "Gradient Descent(26/49): objective=15.3858878688294\n",
      "Gradient Descent(27/49): objective=15.3858878688294\n",
      "Gradient Descent(28/49): objective=15.3858878688294\n",
      "Gradient Descent(29/49): objective=15.3858878688294\n",
      "Gradient Descent(30/49): objective=15.3858878688294\n",
      "Gradient Descent(31/49): objective=15.3858878688294\n",
      "Gradient Descent(32/49): objective=15.3858878688294\n",
      "Gradient Descent(33/49): objective=15.3858878688294\n",
      "Gradient Descent(34/49): objective=15.3858878688294\n",
      "Gradient Descent(35/49): objective=15.3858878688294\n",
      "Gradient Descent(36/49): objective=15.3858878688294\n",
      "Gradient Descent(37/49): objective=15.3858878688294\n",
      "Gradient Descent(38/49): objective=15.3858878688294\n",
      "Gradient Descent(39/49): objective=15.3858878688294\n",
      "Gradient Descent(40/49): objective=15.3858878688294\n",
      "Gradient Descent(41/49): objective=15.3858878688294\n",
      "Gradient Descent(42/49): objective=15.3858878688294\n",
      "Gradient Descent(43/49): objective=15.3858878688294\n",
      "Gradient Descent(44/49): objective=15.3858878688294\n",
      "Gradient Descent(45/49): objective=15.3858878688294\n",
      "Gradient Descent(46/49): objective=15.3858878688294\n",
      "Gradient Descent(47/49): objective=15.3858878688294\n",
      "Gradient Descent(48/49): objective=15.3858878688294\n",
      "Gradient Descent(49/49): objective=15.3858878688294\n",
      "Gradient Descent: execution time=0.007 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: a better learning rate using the smoothness of f\n",
    "# ***************************************************\n",
    "gamma = 1/calculate_L(b,A)\n",
    "#raise NotImplementedError\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_objectives, gradient_xs = gradient_descent(b, A, x_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time visualization with a better learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6aaaa6f3a3487eb92517a012714ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_figure(n_iter):\n",
    "    # Generate grid data for visualization (parameters to be swept and best combination)\n",
    "    grid_x0, grid_x1 = generate_w(num_intervals=10)\n",
    "    grid_objectives = grid_search(b, A, grid_x0, grid_x1)\n",
    "    obj_star, x0_star, x1_star = get_best_parameters(grid_x0, grid_x1, grid_objectives)\n",
    "    \n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_objectives, gradient_xs, grid_objectives, grid_x0, grid_x1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_xs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading more complex data\n",
    "The data is taken from https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"Concrete_Data.csv\",delimiter=\",\")\n",
    "\n",
    "A = data[:,:-1]\n",
    "b = data[:,-1]\n",
    "A, mean_A, std_A = standardize(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples n =  1030\n",
      "Dimension of each sample d =  8\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples n = ', b.shape[0])\n",
    "print('Dimension of each sample d = ', A.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assuming bounded gradients\n",
    "Assume we are moving in a bounded region $\\|x\\| \\leq 25$ containing all iterates (and we assume $\\|x-x^\\star\\| \\leq 25$ as well, for simplicity). Then by $\\nabla f(x) = \\frac{1}{n}A^\\top (Ax - b)$, one can see that $f$ is Lipschitz over that bounded region, with Lipschitz constant $\\|\\nabla f(x)\\| \\leq \\frac{1}{n} (\\|A^\\top A\\|\\|x\\| + \\|A^\\top b\\|)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: Compute the bound on the gradient norm\n",
    "# ***************************************************\n",
    "grad_norm_bound = 1/A.shape[0]*(np.linalg.norm(A.T@A,2)*25+np.linalg.norm(A.T@b,2))\n",
    "#raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the learning rate assuming bounded gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): objective=780.8686016504854\n",
      "Gradient Descent(1/49): objective=771.9451380356\n",
      "Gradient Descent(2/49): objective=764.0577873681689\n",
      "Gradient Descent(3/49): objective=757.0806435792373\n",
      "Gradient Descent(4/49): objective=750.9036210736357\n",
      "Gradient Descent(5/49): objective=745.4304063496157\n",
      "Gradient Descent(6/49): objective=740.5766831862035\n",
      "Gradient Descent(7/49): objective=736.2685935205593\n",
      "Gradient Descent(8/49): objective=732.4414016205171\n",
      "Gradient Descent(9/49): objective=729.0383338025285\n",
      "Gradient Descent(10/49): objective=726.0095698886381\n",
      "Gradient Descent(11/49): objective=723.3113659504753\n",
      "Gradient Descent(12/49): objective=720.9052907468908\n",
      "Gradient Descent(13/49): objective=718.7575607023603\n",
      "Gradient Descent(14/49): objective=716.8384603602686\n",
      "Gradient Descent(15/49): objective=715.1218370326862\n",
      "Gradient Descent(16/49): objective=713.5846599014745\n",
      "Gradient Descent(17/49): objective=712.206635142504\n",
      "Gradient Descent(18/49): objective=710.9698697774038\n",
      "Gradient Descent(19/49): objective=709.8585779325339\n",
      "Gradient Descent(20/49): objective=708.8588240256167\n",
      "Gradient Descent(21/49): objective=707.9582981259695\n",
      "Gradient Descent(22/49): objective=707.1461193609596\n",
      "Gradient Descent(23/49): objective=706.4126637831231\n",
      "Gradient Descent(24/49): objective=705.749413581208\n",
      "Gradient Descent(25/49): objective=705.1488249244085\n",
      "Gradient Descent(26/49): objective=704.6042120808993\n",
      "Gradient Descent(27/49): objective=704.1096457569055\n",
      "Gradient Descent(28/49): objective=703.6598638673269\n",
      "Gradient Descent(29/49): objective=703.2501931788439\n",
      "Gradient Descent(30/49): objective=702.8764804661781\n",
      "Gradient Descent(31/49): objective=702.535031995811\n",
      "Gradient Descent(32/49): objective=702.2225603024712\n",
      "Gradient Descent(33/49): objective=701.9361373550965\n",
      "Gradient Descent(34/49): objective=701.6731533233591\n",
      "Gradient Descent(35/49): objective=701.4312802554623\n",
      "Gradient Descent(36/49): objective=701.208440064709\n",
      "Gradient Descent(37/49): objective=701.0027762979972\n",
      "Gradient Descent(38/49): objective=700.8126292253705\n",
      "Gradient Descent(39/49): objective=700.6365138472892\n",
      "Gradient Descent(40/49): objective=700.4731004665174\n",
      "Gradient Descent(41/49): objective=700.3211975153511\n",
      "Gradient Descent(42/49): objective=700.1797363672056\n",
      "Gradient Descent(43/49): objective=700.0477578950218\n",
      "Gradient Descent(44/49): objective=699.9244005681861\n",
      "Gradient Descent(45/49): objective=699.8088899052115\n",
      "Gradient Descent(46/49): objective=699.7005291217806\n",
      "Gradient Descent(47/49): objective=699.5986908333042\n",
      "Gradient Descent(48/49): objective=699.5028096882613\n",
      "Gradient Descent(49/49): objective=699.4123758235777\n",
      "Gradient Descent: execution time=0.003 seconds\n"
     ]
    }
   ],
   "source": [
    "max_iters = 50\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: Compute learning rate based on bounded gradient\n",
    "# ***************************************************\n",
    "gamma = 25/(grad_norm_bound*np.sqrt(max_iters))\n",
    "#raise NotImplementedError\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "bd_gradient_objectives, bd_gradient_xs = gradient_descent(b, A, x_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n",
    "# Averaging the iterates as is the case for bounded gradients case\n",
    "bd_gradient_objectives_averaged = []\n",
    "for i in range(len(bd_gradient_xs)):\n",
    "    if i > 0:\n",
    "        bd_gradient_xs[i] = (i * bd_gradient_xs[i-1] + bd_gradient_xs[i])/(i + 1)\n",
    "    grad, err = compute_gradient(b, A, bd_gradient_xs[i])\n",
    "    obj = calculate_objective(err)\n",
    "    bd_gradient_objectives_averaged.append(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent using smoothness\n",
    "Fill in the learning rate using smoothness of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 50\n",
    "\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: a better learning rate using the smoothness of f\n",
    "# ***************************************************\n",
    "gamma = \n",
    "raise NotImplementedError\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_objectives, gradient_xs = gradient_descent(b, A, x_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Evolution of the Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Objective Function')\n",
    "#plt.yscale(\"log\")\n",
    "plt.plot(range(len(gradient_objectives)), gradient_objectives,'r', label='gradient descent with 1/L stepsize')\n",
    "plt.plot(range(len(bd_gradient_objectives)), bd_gradient_objectives,'b', label='gradient descent assuming bounded gradients')\n",
    "plt.plot(range(len(bd_gradient_objectives_averaged)), bd_gradient_objectives_averaged,'g', label='gradient descent assuming bounded gradients with averaged iterates')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
